{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     art_id  weight  lvl0_id  lvl1_id  lvl2_id  lvl3_id\n",
      "0    192584    0.21    16752     1677     7263    15232\n",
      "1    310536    1.00    16352       34    16258      574\n",
      "2  15321862    0.25    16722      655    16094    16092\n",
      "3    114853    0.60     2135     1953     1962     7961\n",
      "4  16522966    0.50      679      912      970    14862\n",
      "Shape\n",
      "(12382, 6)\n",
      "Unique\n",
      "art_id  \t 12382\n",
      "weight  \t 486\n",
      "lvl0_id \t 35\n",
      "lvl1_id \t 185\n",
      "lvl2_id \t 693\n",
      "lvl3_id \t 1511\n",
      "Shape\n",
      "(12382, 7)\n",
      "Unique\n",
      "art_id  \t 12382\n",
      "weight  \t 486\n",
      "lvl0_id \t 35\n",
      "lvl1_id \t 185\n",
      "lvl2_id \t 693\n",
      "lvl3_id \t 1511\n",
      "art     \t 1511\n",
      "     art_id  weight  lvl0_id  lvl1_id  lvl2_id  lvl3_id        art\n",
      "0    192584    0.21    16752     1677     7263    15232  134847292\n",
      "1    310536    1.00    16352       34    16258      574  131155334\n",
      "2  15321862    0.25    16722      655    16094    16092  134375972\n",
      "3    114853    0.60     2135     1953     1962     7961   17908401\n",
      "4  16522966    0.50      679      912      970    14862    5831062\n"
     ]
    }
   ],
   "source": [
    "art = pd.read_csv('clients_art_wo_names.csv')\n",
    "# Я предварительно поменял в исходном файле десятичный разделитель с запятой на точку\n",
    "print(art[:5])\n",
    "print('Shape')\n",
    "print(art.shape)\n",
    "print('Unique')\n",
    "for column in art.columns:\n",
    "    print(\"{:8s}\\t\".format(column), art[column].unique().shape[0])\n",
    "# Считаем, что артикул art_id - просто случайное число, а из кодов групп в дереве товаров\n",
    "# можно получить какую-то метрику. Я предполагаю, что вода, хлеб и крупа не окажутся \n",
    "# одновременно в одной группе товаров третьего уровня, а условные вода \"Шишкин лес\" и \"Святой\n",
    "# источник окажутся в одной группе lvl2_id, а lvl3_id у них будет отличаться. Задам вручную вес\n",
    "# для каждого уровня\n",
    "NN = 20 # Проверено, коллизий нет\n",
    "art['art'] = NN**3 * art['lvl0_id'] + NN**2 * art['lvl1_id'] + NN * art['lvl2_id'] + art['lvl3_id']\n",
    "print('Shape')\n",
    "print(art.shape)\n",
    "print('Unique')\n",
    "for column in art.columns:\n",
    "    print(\"{:8s}\\t\".format(column), art[column].unique().shape[0])\n",
    "print(art[:5])\n",
    "#for i in range(4):\n",
    "#    print(art['lvl{}_id'.format(i)].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "       day_id    art_id  whs_id       txn_id  sale_qnty  reg_opsum  \\\n",
      "0  2018-02-04  16550635   26517  14914458657      1.000     85.900   \n",
      "1  2018-02-03    243028   26517  14905137106      1.000    149.900   \n",
      "2  2018-02-03    327275   26517  14895612196      0.173     60.533   \n",
      "\n",
      "   fact_opsum  mission_id  client_id  \n",
      "0      85.900          -1  116871314  \n",
      "1     149.900          -1  126493766  \n",
      "2      60.533          -1  105675535  \n",
      "Test\n",
      "       day_id    art_id  whs_id       txn_id  sale_qnty  reg_opsum  \\\n",
      "0  2018-04-01    275845   26517  15578368583        1.0       24.9   \n",
      "1  2018-04-01  16751680   26517  15578368583        1.0       29.9   \n",
      "2  2018-04-01  15333128   26517  15578368583        1.0       19.9   \n",
      "\n",
      "   fact_opsum  mission_id  \n",
      "0        24.9         220  \n",
      "1        29.9         220  \n",
      "2        19.9         220  \n",
      "----------------------------------------\n",
      "Unique values\n",
      "Column\t\tTrain\t\tTest\n",
      "Shape\t\t (8093, 9) \t (3581, 8)\n",
      "day_id  \t 59 \t\t 30\n",
      "art_id  \t 2151 \t\t 1304\n",
      "whs_id  \t 1 \t\t 1\n",
      "txn_id  \t 2043 \t\t 950\n",
      "sale_qnty\t 678 \t\t 375\n",
      "reg_opsum\t 2299 \t\t 1063\n",
      "fact_opsum\t 2591 \t\t 1163\n",
      "mission_id\t 20 \t\t 20\n",
      "\n",
      "client_id\t 69\n",
      "----------------------------------------\n",
      "Test in train\n",
      "day_id  \t\t []\n",
      "art_id  \t\t [2977]\n",
      "whs_id  \t\t [3581]\n",
      "txn_id  \t\t []\n",
      "sale_qnty\t\t [3387]\n",
      "reg_opsum\t\t [2803]\n",
      "fact_opsum\t\t [2767]\n",
      "mission_id\t\t [3576]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('clients_train.csv')\n",
    "test = pd.read_csv('clients_test_cut.csv')\n",
    "print('Train')\n",
    "print(train[:3])\n",
    "print('Test')\n",
    "print(test[:3])\n",
    "print('----------------------------------------')\n",
    "print('Unique values')\n",
    "print('Column\\t\\tTrain\\t\\tTest')\n",
    "print('Shape\\t\\t', train.shape, '\\t', test.shape)\n",
    "for column in test.columns:\n",
    "    print(\"{:8s}\\t\".format(column), train[column].unique().shape[0], '\\t\\t', test[column].unique().shape[0])\n",
    "print(\"\\n{:8s}\\t\".format('client_id'), train['client_id'].unique().shape[0])\n",
    "print('----------------------------------------')\n",
    "# Посмотрим, есть ли пересечения в данных\n",
    "print('Test in train')\n",
    "for column in test.columns:\n",
    "    counts = test[column].isin(train[column]).value_counts()\n",
    "    print(\"{:8s}\\t\\t\".format(column), counts[counts.index == True].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in train\n",
      "day_id  \t\t []\n",
      "art_id  \t\t [2977]\n",
      "txn_id  \t\t []\n",
      "sale_qnty\t\t [3387]\n",
      "reg_opsum\t\t [2803]\n",
      "fact_opsum\t\t [2767]\n",
      "mission_id\t\t [3576]\n",
      "weight  \t\t [3551]\n",
      "lvl0_id \t\t [3581]\n",
      "lvl1_id \t\t [3571]\n",
      "lvl2_id \t\t [3551]\n",
      "lvl3_id \t\t [3502]\n",
      "art     \t\t [3502]\n",
      "qty     \t\t [3399]\n",
      "prod_disc\t\t [2849]\n",
      "dayofweek\t\t [3581]\n",
      "----------------------------------------\n",
      "Unique values\n",
      "Column\t\tTrain\t\tTest\n",
      "Shape\t\t (8093, 17) \t (3581, 16)\n",
      "day_id  \t 59 \t\t 30\n",
      "art_id  \t 2151 \t\t 1304\n",
      "txn_id  \t 2043 \t\t 950\n",
      "sale_qnty\t 678 \t\t 375\n",
      "reg_opsum\t 2299 \t\t 1063\n",
      "fact_opsum\t 2591 \t\t 1163\n",
      "mission_id\t 20 \t\t 20\n",
      "weight  \t 211 \t\t 175\n",
      "lvl0_id \t 34 \t\t 33\n",
      "lvl1_id \t 141 \t\t 127\n",
      "lvl2_id \t 403 \t\t 312\n",
      "lvl3_id \t 667 \t\t 470\n",
      "art     \t 667 \t\t 470\n",
      "qty     \t 820 \t\t 508\n",
      "prod_disc\t 1618 \t\t 665\n",
      "dayofweek\t 7 \t\t 7\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('clients_train.csv')\n",
    "test = pd.read_csv('clients_test_cut.csv')\n",
    "\n",
    "def gen_features(X):\n",
    "    X = X.copy()\n",
    "    X = X.join(art.set_index('art_id'), on = 'art_id')\n",
    "    # Артикул можно бы и удалить, но градиентный бустинг достаточно устойчив\n",
    "    # X.drop(['art_id', 'lvl0_id', 'lvl1_id', 'lvl2_id', 'lvl3_id'], inplace = True, axis = 1)\n",
    "    \n",
    "    # Одно число в столбце. Для обучения модели оно не поможет\n",
    "    X.drop('whs_id', inplace = True, axis = 1)\n",
    "    # Так как и количество, и вес могут быть дробными, введем еще одну переменную\n",
    "    X['qty'] = X['sale_qnty'].multiply(X['weight'])\n",
    "    # X.drop(['sale_qnty', 'weight'], inplace = True, axis = 1)\n",
    "    X['prod_disc'] = (X['reg_opsum'] - X['fact_opsum']).divide(X['reg_opsum'])\n",
    "    X['day_id'] = X['day_id'].map(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "    X['dayofweek'] = X['day_id'].map(lambda x: x.dayofweek)\n",
    "    X['day_id'] = X['day_id'].map(lambda x: x.timestamp())\n",
    "    return X\n",
    "\n",
    "X_train, X_test = gen_features(train), gen_features(test)\n",
    "# Fill NaN in 'prod_disc'\n",
    "X_train.fillna(value = 1, inplace = True)\n",
    "X_test.fillna(value = 1, inplace = True)\n",
    "#my_train = X_train.copy()\n",
    "\n",
    "#y_train = X_train['client_id']\n",
    "#X_train.drop('client_id', inplace = True, axis = 1)\n",
    "print('Test in train')\n",
    "for column in X_test.columns:\n",
    "    counts = X_test[column].isin(X_train[column]).value_counts()\n",
    "    print(\"{:8s}\\t\\t\".format(column), counts[counts.index == True].values)\n",
    "print('----------------------------------------')\n",
    "print('Unique values')\n",
    "print('Column\\t\\tTrain\\t\\tTest')\n",
    "print('Shape\\t\\t', X_train.shape, '\\t', X_test.shape)\n",
    "for column in X_test.columns:\n",
    "    print(\"{:8s}\\t\".format(column), X_train[column].unique().shape[0], '\\t\\t', X_test[column].unique().shape[0])\n",
    "print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_new_data(X_train, X_test):\n",
    "    # Стоило написать дополнительную функцию для формирования train и test. Было бы проще и более читаемо\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "    # Округляем количество покупок вверх до целого, чтобы потом посчитать количество товаров в чеке\n",
    "    X_train['qty_int'] = X_train['sale_qnty'].map(ceil) \n",
    "    X_test['qty_int'] = X_test['sale_qnty'].map(ceil) \n",
    "    # Просуммируем количество товаров, определим общую сумму чека\n",
    "    train = X_train.groupby('txn_id')['qty', 'reg_opsum', 'fact_opsum', 'qty_int'].sum()\n",
    "    test = X_test.groupby('txn_id')['qty', 'reg_opsum', 'fact_opsum', 'qty_int'].sum()\n",
    "    # Добавим дату и день недели к данным\n",
    "    train = train.join(X_train.set_index('txn_id')[['day_id', 'dayofweek', 'mission_id']])\n",
    "    test = test.join(X_train.set_index('txn_id')[['day_id', 'dayofweek', 'mission_id']])\n",
    "    # Посчитаем скидку по чеку\n",
    "    train['prod_disc'] = (train['reg_opsum'] - train['fact_opsum']).divide(train['reg_opsum'])\n",
    "    test['prod_disc'] = (test['reg_opsum'] - test['fact_opsum']).divide(test['reg_opsum'])\n",
    "    # Определим среднюю стоимость товара в чеке\n",
    "    train['avg'] = train['reg_opsum'].divide(train['qty_int'])\n",
    "    test['avg'] = test['reg_opsum'].divide(test['qty_int'])\n",
    "\n",
    "    y_train = X_train[['txn_id', 'client_id']].drop_duplicates().set_index('txn_id')\n",
    "    \n",
    "       \n",
    "    ohe = OneHotEncoder(sparse = False)\n",
    "    temp = pd.concat([X_train[['lvl0_id']], X_test[['lvl0_id']]])['lvl0_id'].map(str)\n",
    "    ohe.fit(temp.values.reshape(-1, 1))\n",
    "    ohe_train = pd.DataFrame(ohe.transform(X_train['lvl0_id'].values.reshape(-1, 1)), index = X_train.index).join(X_train['txn_id'])\n",
    "    ohe_test = pd.DataFrame(ohe.transform(X_test['lvl0_id'].values.reshape(-1, 1)), index = X_test.index).join(X_test['txn_id'])\n",
    "    ohe_train = ohe_train.groupby('txn_id').sum()\n",
    "    ohe_test = ohe_test.groupby('txn_id').sum()\n",
    "    ohe_train.columns = ['lvl0_' + str(int(i)) for i in range(ohe_train.shape[1])]\n",
    "    ohe_test.columns = ['lvl0_' + str(int(i)) for i in range(ohe_test.shape[1])]\n",
    "    #train = train.join(ohe_train)\n",
    "    #test = test.join(ohe_test)\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse = False)\n",
    "    temp = pd.concat([X_train[['lvl1_id']], X_test[['lvl1_id']]])['lvl1_id'].map(str)\n",
    "    ohe.fit(temp.values.reshape(-1, 1))\n",
    "    ohe_train = pd.DataFrame(ohe.transform(X_train['lvl1_id'].values.reshape(-1, 1)), index = X_train.index).join(X_train['txn_id'])\n",
    "    ohe_test = pd.DataFrame(ohe.transform(X_test['lvl1_id'].values.reshape(-1, 1)), index = X_test.index).join(X_test['txn_id'])\n",
    "    ohe_train = ohe_train.groupby('txn_id').sum()\n",
    "    ohe_test = ohe_test.groupby('txn_id').sum()\n",
    "    ohe_train.columns = ['lvl1_' + str(int(i)) for i in range(ohe_train.shape[1])]\n",
    "    ohe_test.columns = ['lvl1_' + str(int(i)) for i in range(ohe_test.shape[1])]\n",
    "    #train = train.join(ohe_train)\n",
    "    #test = test.join(ohe_test)\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse = False)\n",
    "    temp = pd.concat([X_train[['lvl2_id']], X_test[['lvl2_id']]])['lvl2_id'].map(str)\n",
    "    ohe.fit(temp.values.reshape(-1, 1))\n",
    "    ohe_train = pd.DataFrame(ohe.transform(X_train['lvl2_id'].values.reshape(-1, 1)), index = X_train.index).join(X_train['txn_id'])\n",
    "    ohe_test = pd.DataFrame(ohe.transform(X_test['lvl2_id'].values.reshape(-1, 1)), index = X_test.index).join(X_test['txn_id'])\n",
    "    ohe_train = ohe_train.groupby('txn_id').sum()\n",
    "    ohe_test = ohe_test.groupby('txn_id').sum()\n",
    "    ohe_train.columns = ['lvl2_' + str(int(i)) for i in range(ohe_train.shape[1])]\n",
    "    ohe_test.columns = ['lvl2_' + str(int(i)) for i in range(ohe_test.shape[1])]\n",
    "    #train = train.join(ohe_train)\n",
    "    #test = test.join(ohe_test)\n",
    "    \n",
    "    # Удалим все дубликаты\n",
    "    train = train.reset_index().drop_duplicates()\n",
    "    train = train.set_index('txn_id')\n",
    "    test = test.reset_index().drop_duplicates()\n",
    "    test = test.set_index('txn_id')\n",
    "    train = train.sort_index()\n",
    "    test = test.sort_index()\n",
    "    train.fillna(value = 1, inplace = True)\n",
    "    test.fillna(value = 1, inplace = True)\n",
    "    return train, y_train, test\n",
    "\n",
    "train, y_train, test = mk_new_data(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "colors = [int(i % y_train['client_id'].unique().shape[0]) for i in y_train.reset_index(drop = True).index]\n",
    "for i in train.columns:\n",
    "    for j in train.columns:\n",
    "        pylab.scatter(train[i], train[j], c=colors)\n",
    "        pylab.xlabel(i)\n",
    "        pylab.ylabel(i)\n",
    "        pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
